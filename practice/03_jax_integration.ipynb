{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6787c22",
   "metadata": {},
   "source": [
    "# JAX Integration in NetKet: High-Performance Automatic Differentiation\n",
    "\n",
    "This notebook explores how NetKet leverages JAX for high-performance automatic differentiation and just-in-time compilation, making quantum many-body calculations efficient and scalable.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to JAX](#introduction)\n",
    "2. [Automatic Differentiation Basics](#autodiff)\n",
    "3. [JIT Compilation](#jit)\n",
    "4. [Vectorization with vmap](#vmap)\n",
    "5. [Parallelization with pmap](#pmap)\n",
    "6. [Custom Gradients](#custom-gradients)\n",
    "7. [Memory and Performance Optimization](#optimization)\n",
    "8. [NetKet-specific JAX Usage](#netket-jax)\n",
    "9. [Advanced Topics](#advanced)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048eed9",
   "metadata": {},
   "source": [
    "## 1. Introduction to JAX {#introduction}\n",
    "\n",
    "JAX is NumPy-compatible library that provides:\n",
    "- **Automatic differentiation**: Forward and reverse mode\n",
    "- **JIT compilation**: XLA-powered optimization\n",
    "- **Vectorization**: Automatic batching with `vmap`\n",
    "- **Parallelization**: Multi-device computation with `pmap`\n",
    "\n",
    "These features make JAX ideal for machine learning and scientific computing, especially for quantum many-body problems where gradients and performance are crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import netket as nk\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure JAX\n",
    "jax.config.update('jax_platform_name', 'cpu')  # Use CPU for this tutorial\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "\n",
    "# Set random seeds\n",
    "key = jax.random.PRNGKey(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d623f",
   "metadata": {},
   "source": [
    "## 2. Automatic Differentiation Basics {#autodiff}\n",
    "\n",
    "Automatic differentiation is fundamental to neural quantum states optimization. JAX provides both forward and reverse mode AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec687ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic automatic differentiation examples\n",
    "\n",
    "# Simple function\n",
    "def f(x):\n",
    "    return x**3 + 2*x**2 - x + 1\n",
    "\n",
    "# Compute derivative\n",
    "f_grad = jax.grad(f)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {f_grad(x)}\")\n",
    "print(f\"Analytical f'({x}) = {3*x**2 + 4*x - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector function example\n",
    "def vector_func(params):\n",
    "    \"\"\"Function that takes a parameter vector\"\"\"\n",
    "    a, b, c = params\n",
    "    return a**2 + b*c + jnp.sin(a*b)\n",
    "\n",
    "# Gradient with respect to all parameters\n",
    "vector_grad = jax.grad(vector_func)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "print(f\"Function value: {vector_func(params)}\")\n",
    "print(f\"Gradient: {vector_grad(params)}\")\n",
    "\n",
    "# Verify with finite differences\n",
    "eps = 1e-7\n",
    "finite_diff_grad = jnp.array([\n",
    "    (vector_func(params + eps * jnp.array([1, 0, 0])) - \n",
    "     vector_func(params - eps * jnp.array([1, 0, 0]))) / (2 * eps),\n",
    "    (vector_func(params + eps * jnp.array([0, 1, 0])) - \n",
    "     vector_func(params - eps * jnp.array([0, 1, 0]))) / (2 * eps),\n",
    "    (vector_func(params + eps * jnp.array([0, 0, 1])) - \n",
    "     vector_func(params - eps * jnp.array([0, 0, 1]))) / (2 * eps)\n",
    "])\n",
    "print(f\"Finite difference: {finite_diff_grad}\")\n",
    "print(f\"Difference: {jnp.abs(vector_grad(params) - finite_diff_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fe0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-order derivatives\n",
    "def quadratic(x):\n",
    "    return x**4 - 2*x**3 + x**2\n",
    "\n",
    "# First derivative\n",
    "first_deriv = jax.grad(quadratic)\n",
    "# Second derivative\n",
    "second_deriv = jax.grad(first_deriv)\n",
    "# Third derivative\n",
    "third_deriv = jax.grad(second_deriv)\n",
    "\n",
    "x = 1.5\n",
    "print(f\"f({x}) = {quadratic(x)}\")\n",
    "print(f\"f'({x}) = {first_deriv(x)}\")\n",
    "print(f\"f''({x}) = {second_deriv(x)}\")\n",
    "print(f\"f'''({x}) = {third_deriv(x)}\")\n",
    "\n",
    "# Compare with analytical results\n",
    "print(f\"\\nAnalytical:\")\n",
    "print(f\"f'({x}) = {4*x**3 - 6*x**2 + 2*x}\")\n",
    "print(f\"f''({x}) = {12*x**2 - 12*x + 2}\")\n",
    "print(f\"f'''({x}) = {24*x - 12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42a9b7",
   "metadata": {},
   "source": [
    "## 3. JIT Compilation {#jit}\n",
    "\n",
    "Just-in-time compilation with XLA can dramatically speed up computations by optimizing the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT compilation example\n",
    "def slow_function(x):\n",
    "    \"\"\"A function with many operations\"\"\"\n",
    "    y = x\n",
    "    for i in range(100):\n",
    "        y = jnp.sin(y) + jnp.cos(y) * jnp.exp(-y/10)\n",
    "    return y\n",
    "\n",
    "# JIT-compiled version\n",
    "fast_function = jax.jit(slow_function)\n",
    "\n",
    "# Test input\n",
    "x = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Time the functions\n",
    "print(\"Timing comparison:\")\n",
    "\n",
    "# Regular function (first call)\n",
    "start = time.time()\n",
    "result_slow = slow_function(x)\n",
    "time_slow = time.time() - start\n",
    "print(f\"Regular function: {time_slow:.4f} seconds\")\n",
    "\n",
    "# JIT function (first call - includes compilation)\n",
    "start = time.time()\n",
    "result_fast = fast_function(x)\n",
    "time_fast_first = time.time() - start\n",
    "print(f\"JIT function (first call): {time_fast_first:.4f} seconds\")\n",
    "\n",
    "# JIT function (subsequent calls)\n",
    "start = time.time()\n",
    "result_fast = fast_function(x)\n",
    "time_fast_subsequent = time.time() - start\n",
    "print(f\"JIT function (subsequent): {time_fast_subsequent:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup: {time_slow / time_fast_subsequent:.1f}x\")\n",
    "print(f\"Results match: {jnp.allclose(result_slow, result_fast)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations with JIT\n",
    "def matrix_operations(A, B):\n",
    "    \"\"\"Complex matrix operations\"\"\"\n",
    "    C = jnp.dot(A, B)\n",
    "    D = jnp.linalg.inv(C + jnp.eye(C.shape[0]) * 1e-6)\n",
    "    E = jnp.dot(D, A.T)\n",
    "    return jnp.trace(E)\n",
    "\n",
    "# JIT-compiled version\n",
    "matrix_operations_jit = jax.jit(matrix_operations)\n",
    "\n",
    "# Test matrices\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "A = jax.random.normal(subkey1, (100, 100))\n",
    "B = jax.random.normal(subkey2, (100, 100))\n",
    "\n",
    "# Benchmark\n",
    "n_runs = 10\n",
    "\n",
    "# Regular function\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    result_regular = matrix_operations(A, B)\n",
    "time_regular = (time.time() - start) / n_runs\n",
    "\n",
    "# JIT function (warm up first)\n",
    "_ = matrix_operations_jit(A, B)\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    result_jit = matrix_operations_jit(A, B)\n",
    "time_jit = (time.time() - start) / n_runs\n",
    "\n",
    "print(f\"Matrix operations benchmark ({n_runs} runs):\")\n",
    "print(f\"Regular: {time_regular:.4f} seconds per run\")\n",
    "print(f\"JIT: {time_jit:.4f} seconds per run\")\n",
    "print(f\"Speedup: {time_regular / time_jit:.1f}x\")\n",
    "print(f\"Results match: {jnp.allclose(result_regular, result_jit)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e41ba3",
   "metadata": {},
   "source": [
    "## 4. Vectorization with vmap {#vmap}\n",
    "\n",
    "`vmap` automatically vectorizes functions, eliminating the need for explicit loops and improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c28fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic vmap example\n",
    "def single_computation(x):\n",
    "    \"\"\"Function that works on a single input\"\"\"\n",
    "    return jnp.sin(x**2) + jnp.cos(x**3)\n",
    "\n",
    "# Vectorized version\n",
    "vectorized_computation = jax.vmap(single_computation)\n",
    "\n",
    "# Test data\n",
    "x_single = 2.0\n",
    "x_batch = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(f\"Single input: {single_computation(x_single)}\")\n",
    "print(f\"Batch input: {vectorized_computation(x_batch)}\")\n",
    "\n",
    "# Compare with manual loop\n",
    "manual_result = jnp.array([single_computation(x) for x in x_batch])\n",
    "print(f\"Manual loop: {manual_result}\")\n",
    "print(f\"Results match: {jnp.allclose(vectorized_computation(x_batch), manual_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced vmap with multiple arguments\n",
    "def pairwise_distance(x1, x2):\n",
    "    \"\"\"Compute distance between two points\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x1 - x2)**2))\n",
    "\n",
    "# Vectorize over first argument only\n",
    "distances_from_point = jax.vmap(pairwise_distance, in_axes=(0, None))\n",
    "\n",
    "# Vectorize over both arguments\n",
    "elementwise_distances = jax.vmap(pairwise_distance, in_axes=(0, 0))\n",
    "\n",
    "# Test data\n",
    "key, subkey = jax.random.split(key)\n",
    "points = jax.random.normal(subkey, (5, 3))  # 5 points in 3D\n",
    "reference_point = jnp.array([0.0, 0.0, 0.0])\n",
    "\n",
    "print(\"Points:\")\n",
    "print(points)\n",
    "print(f\"\\nDistances from origin: {distances_from_point(points, reference_point)}\")\n",
    "print(f\"Pairwise distances: {elementwise_distances(points[:3], points[2:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: vmap vs loops\n",
    "def complex_function(params):\n",
    "    \"\"\"A more complex function for benchmarking\"\"\"\n",
    "    a, b, c = params\n",
    "    result = 0.0\n",
    "    for i in range(10):\n",
    "        result += jnp.sin(a * i) * jnp.cos(b * i) * jnp.exp(-c * i / 10)\n",
    "    return result\n",
    "\n",
    "# Create test data\n",
    "key, subkey = jax.random.split(key)\n",
    "param_batch = jax.random.normal(subkey, (1000, 3))\n",
    "\n",
    "# Manual loop version\n",
    "def manual_batch(param_batch):\n",
    "    results = []\n",
    "    for params in param_batch:\n",
    "        results.append(complex_function(params))\n",
    "    return jnp.array(results)\n",
    "\n",
    "# Vectorized version\n",
    "vectorized_batch = jax.vmap(complex_function)\n",
    "\n",
    "# JIT-compiled versions\n",
    "manual_batch_jit = jax.jit(manual_batch)\n",
    "vectorized_batch_jit = jax.jit(vectorized_batch)\n",
    "\n",
    "# Warm up\n",
    "_ = manual_batch_jit(param_batch)\n",
    "_ = vectorized_batch_jit(param_batch)\n",
    "\n",
    "# Benchmark\n",
    "n_runs = 10\n",
    "\n",
    "# Manual loop\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    result_manual = manual_batch_jit(param_batch)\n",
    "time_manual = (time.time() - start) / n_runs\n",
    "\n",
    "# Vectorized\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    result_vectorized = vectorized_batch_jit(param_batch)\n",
    "time_vectorized = (time.time() - start) / n_runs\n",
    "\n",
    "print(f\"Batch processing benchmark ({param_batch.shape[0]} items, {n_runs} runs):\")\n",
    "print(f\"Manual loop (JIT): {time_manual:.4f} seconds per run\")\n",
    "print(f\"Vectorized (JIT): {time_vectorized:.4f} seconds per run\")\n",
    "print(f\"Speedup: {time_manual / time_vectorized:.1f}x\")\n",
    "print(f\"Results match: {jnp.allclose(result_manual, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32b256",
   "metadata": {},
   "source": [
    "## 5. Parallelization with pmap {#pmap}\n",
    "\n",
    "`pmap` enables parallel computation across multiple devices (CPUs or GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available devices: {len(devices)}\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"  Device {i}: {device}\")\n",
    "\n",
    "if len(devices) > 1:\n",
    "    print(\"\\nMultiple devices available - demonstrating pmap\")\n",
    "    \n",
    "    def compute_sum_of_squares(x):\n",
    "        \"\"\"Simple computation for parallel execution\"\"\"\n",
    "        return jnp.sum(x**2)\n",
    "    \n",
    "    # Parallel version\n",
    "    parallel_compute = jax.pmap(compute_sum_of_squares)\n",
    "    \n",
    "    # Create data that can be split across devices\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # Shape: (n_devices, batch_size_per_device, features)\n",
    "    data = jax.random.normal(subkey, (len(devices), 100, 50))\n",
    "    \n",
    "    # Sequential computation\n",
    "    start = time.time()\n",
    "    sequential_results = jnp.array([compute_sum_of_squares(data[i]) for i in range(len(devices))])\n",
    "    time_sequential = time.time() - start\n",
    "    \n",
    "    # Parallel computation\n",
    "    start = time.time()\n",
    "    parallel_results = parallel_compute(data)\n",
    "    time_parallel = time.time() - start\n",
    "    \n",
    "    print(f\"Sequential time: {time_sequential:.4f} seconds\")\n",
    "    print(f\"Parallel time: {time_parallel:.4f} seconds\")\n",
    "    print(f\"Speedup: {time_sequential / time_parallel:.1f}x\")\n",
    "    print(f\"Results match: {jnp.allclose(sequential_results, parallel_results)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nOnly single device available - pmap would not provide speedup\")\n",
    "    print(\"pmap is most beneficial with multiple GPUs or CPU cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d6526",
   "metadata": {},
   "source": [
    "## 6. Custom Gradients {#custom-gradients}\n",
    "\n",
    "Sometimes we need custom gradient rules for numerical stability or efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom gradient for numerical stability\n",
    "@jax.custom_jvp\n",
    "def safe_log(x):\n",
    "    \"\"\"Logarithm with custom gradient for numerical stability\"\"\"\n",
    "    return jnp.log(jnp.maximum(x, 1e-10))\n",
    "\n",
    "@safe_log.defjvp\n",
    "def safe_log_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    # Custom gradient: 1/max(x, eps) instead of 1/x\n",
    "    primal_out = safe_log(x)\n",
    "    tangent_out = x_dot / jnp.maximum(x, 1e-8)  # More stable denominator\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# Test the custom gradient\n",
    "x_values = jnp.array([1e-12, 1e-8, 1e-4, 1.0, 10.0])\n",
    "\n",
    "print(\"Comparing regular log vs safe_log:\")\n",
    "for x in x_values:\n",
    "    # Regular log and gradient\n",
    "    try:\n",
    "        regular_val = jnp.log(x)\n",
    "        regular_grad = jax.grad(lambda y: jnp.log(y))(x)\n",
    "    except:\n",
    "        regular_val = float('nan')\n",
    "        regular_grad = float('nan')\n",
    "    \n",
    "    # Safe log and gradient\n",
    "    safe_val = safe_log(x)\n",
    "    safe_grad = jax.grad(safe_log)(x)\n",
    "    \n",
    "    print(f\"x = {x:.2e}:\")\n",
    "    print(f\"  Regular: val = {regular_val:.6f}, grad = {regular_grad:.6f}\")\n",
    "    print(f\"  Safe:    val = {safe_val:.6f}, grad = {safe_grad:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom gradient for computational efficiency\n",
    "@jax.custom_vjp\n",
    "def matrix_sqrt(A):\n",
    "    \"\"\"Matrix square root with custom backward pass\"\"\"\n",
    "    # Forward pass: compute matrix square root via eigendecomposition\n",
    "    eigenvals, eigenvecs = jnp.linalg.eigh(A)\n",
    "    sqrt_eigenvals = jnp.sqrt(jnp.maximum(eigenvals, 0))\n",
    "    return eigenvecs @ jnp.diag(sqrt_eigenvals) @ eigenvecs.T\n",
    "\n",
    "def matrix_sqrt_fwd(A):\n",
    "    # Forward pass and save intermediate values\n",
    "    eigenvals, eigenvecs = jnp.linalg.eigh(A)\n",
    "    sqrt_eigenvals = jnp.sqrt(jnp.maximum(eigenvals, 0))\n",
    "    sqrt_A = eigenvecs @ jnp.diag(sqrt_eigenvals) @ eigenvecs.T\n",
    "    return sqrt_A, (eigenvals, eigenvecs, sqrt_eigenvals)\n",
    "\n",
    "def matrix_sqrt_bwd(res, g):\n",
    "    # Custom backward pass using the stored intermediate values\n",
    "    eigenvals, eigenvecs, sqrt_eigenvals = res\n",
    "    # Simplified gradient computation (for demonstration)\n",
    "    # In practice, this would use the Sylvester equation\n",
    "    g_proj = eigenvecs.T @ g @ eigenvecs\n",
    "    sqrt_eigenvals_expanded = sqrt_eigenvals[:, None]\n",
    "    denom = sqrt_eigenvals_expanded + sqrt_eigenvals_expanded.T\n",
    "    denom = jnp.where(denom == 0, 1, denom)  # Avoid division by zero\n",
    "    grad_eigenvals = g_proj / denom\n",
    "    grad_A = eigenvecs @ grad_eigenvals @ eigenvecs.T\n",
    "    return (grad_A,)\n",
    "\n",
    "matrix_sqrt.defvjp(matrix_sqrt_fwd, matrix_sqrt_bwd)\n",
    "\n",
    "# Test the custom matrix square root\n",
    "key, subkey = jax.random.split(key)\n",
    "A = jax.random.normal(subkey, (4, 4))\n",
    "A = A @ A.T + 0.1 * jnp.eye(4)  # Make positive definite\n",
    "\n",
    "sqrt_A = matrix_sqrt(A)\n",
    "reconstructed = sqrt_A @ sqrt_A\n",
    "\n",
    "print(f\"Matrix square root test:\")\n",
    "print(f\"Original matrix A shape: {A.shape}\")\n",
    "print(f\"sqrt(A) @ sqrt(A) ≈ A: {jnp.allclose(reconstructed, A, atol=1e-6)}\")\n",
    "print(f\"Max reconstruction error: {jnp.max(jnp.abs(reconstructed - A))}\")\n",
    "\n",
    "# Test gradient\n",
    "def loss(A):\n",
    "    sqrt_A = matrix_sqrt(A)\n",
    "    return jnp.sum(sqrt_A**2)\n",
    "\n",
    "grad_A = jax.grad(loss)(A)\n",
    "print(f\"Gradient computed successfully: {grad_A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc43a84",
   "metadata": {},
   "source": [
    "## 7. Memory and Performance Optimization {#optimization}\n",
    "\n",
    "JAX provides several tools for optimizing memory usage and computational performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient computation with gradient checkpointing\n",
    "from jax.experimental import host_callback\n",
    "\n",
    "def memory_intensive_function(x):\n",
    "    \"\"\"Function that requires a lot of intermediate memory\"\"\"\n",
    "    # Simulate memory-intensive computation\n",
    "    for i in range(10):\n",
    "        x = jnp.tanh(x @ x.T + x)\n",
    "        x = x / jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Regular gradient computation\n",
    "regular_grad = jax.grad(memory_intensive_function)\n",
    "\n",
    "# Gradient checkpointing version\n",
    "checkpointed_grad = jax.remat(jax.grad(memory_intensive_function))\n",
    "\n",
    "# Test with smaller matrix for demonstration\n",
    "key, subkey = jax.random.split(key)\n",
    "test_matrix = jax.random.normal(subkey, (20, 20))\n",
    "\n",
    "print(\"Memory optimization with gradient checkpointing:\")\n",
    "\n",
    "# Regular gradient\n",
    "start = time.time()\n",
    "grad_regular = regular_grad(test_matrix)\n",
    "time_regular = time.time() - start\n",
    "\n",
    "# Checkpointed gradient\n",
    "start = time.time()\n",
    "grad_checkpointed = checkpointed_grad(test_matrix)\n",
    "time_checkpointed = time.time() - start\n",
    "\n",
    "print(f\"Regular gradient time: {time_regular:.4f} seconds\")\n",
    "print(f\"Checkpointed gradient time: {time_checkpointed:.4f} seconds\")\n",
    "print(f\"Results match: {jnp.allclose(grad_regular, grad_checkpointed, atol=1e-6)}\")\n",
    "print(f\"Note: Checkpointing trades computation for memory (useful for large models)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static vs dynamic shapes\n",
    "def dynamic_computation(x, n):\n",
    "    \"\"\"Computation with dynamic control flow\"\"\"\n",
    "    result = x\n",
    "    for i in range(n):  # Dynamic loop\n",
    "        result = result + jnp.sin(result)\n",
    "    return result\n",
    "\n",
    "def static_computation(x):\n",
    "    \"\"\"Equivalent computation with static control flow\"\"\"\n",
    "    result = x\n",
    "    for i in range(5):  # Static loop\n",
    "        result = result + jnp.sin(result)\n",
    "    return result\n",
    "\n",
    "# JIT compilation works better with static shapes\n",
    "static_jit = jax.jit(static_computation)\n",
    "\n",
    "# For dynamic shapes, we can use partial JIT\n",
    "dynamic_jit = jax.jit(dynamic_computation, static_argnums=(1,))\n",
    "\n",
    "# Test data\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "n = 5\n",
    "\n",
    "print(\"Static vs dynamic shape optimization:\")\n",
    "\n",
    "# Static computation\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result_static = static_jit(x)\n",
    "time_static = time.time() - start\n",
    "\n",
    "# Dynamic computation (with static argument)\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result_dynamic = dynamic_jit(x, n)\n",
    "time_dynamic = time.time() - start\n",
    "\n",
    "print(f\"Static computation: {time_static:.4f} seconds\")\n",
    "print(f\"Dynamic computation: {time_dynamic:.4f} seconds\")\n",
    "print(f\"Results match: {jnp.allclose(result_static, result_dynamic)}\")\n",
    "print(f\"Speedup from static: {time_dynamic / time_static:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6247984",
   "metadata": {},
   "source": [
    "## 8. NetKet-specific JAX Usage {#netket-jax}\n",
    "\n",
    "Let's see how NetKet leverages JAX features for quantum many-body calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a simple quantum system\n",
    "N = 6\n",
    "hilbert = nk.hilbert.Spin(s=1/2, N=N)\n",
    "lattice = nk.graph.Chain(length=N, pbc=True)\n",
    "hamiltonian = nk.operator.Ising(hilbert=hilbert, graph=lattice, J=1.0, h=0.5)\n",
    "\n",
    "# Create RBM model\n",
    "model = nk.models.RBM(alpha=2, dtype=jnp.complex128)\n",
    "\n",
    "print(f\"Quantum system: {N} spins, Hilbert space dimension = {hilbert.n_states}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate JAX transformations in NetKet context\n",
    "\n",
    "# Initialize model parameters\n",
    "key, subkey = jax.random.split(key)\n",
    "sample_input = hilbert.random_state(subkey, size=1)\n",
    "params = model.init(subkey, sample_input)\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "for name, param in params['params'].items():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Function to compute log-amplitude\n",
    "def log_psi(params, sigma):\n",
    "    return model.apply(params, sigma)\n",
    "\n",
    "# Vectorized version for batch evaluation\n",
    "log_psi_batched = jax.vmap(log_psi, in_axes=(None, 0))\n",
    "\n",
    "# Generate batch of configurations\n",
    "key, subkey = jax.random.split(key)\n",
    "batch_configs = hilbert.random_state(subkey, size=100)\n",
    "\n",
    "# Evaluate log-amplitudes\n",
    "log_amplitudes = log_psi_batched(params, batch_configs)\n",
    "print(f\"\\nBatch evaluation:\")\n",
    "print(f\"  Input shape: {batch_configs.shape}\")\n",
    "print(f\"  Output shape: {log_amplitudes.shape}\")\n",
    "print(f\"  Mean log|ψ|: {jnp.mean(jnp.real(log_amplitudes)):.4f}\")\n",
    "print(f\"  Std log|ψ|: {jnp.std(jnp.real(log_amplitudes)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local energy computation with JAX\n",
    "def local_energy(params, sigma, hamiltonian):\n",
    "    \"\"\"Compute local energy E_loc = <σ|H|ψ>/<σ|ψ>\"\"\"\n",
    "    # This is a simplified version - NetKet handles this more efficiently\n",
    "    log_psi_sigma = log_psi(params, sigma)\n",
    "    \n",
    "    # Get matrix elements and connected configurations\n",
    "    mel = hamiltonian.get_conn_flattened(sigma.reshape(1, -1))\n",
    "    configs, matrix_elements = mel[0], mel[1]\n",
    "    \n",
    "    # Compute log-amplitudes for connected configurations\n",
    "    log_psi_connected = log_psi_batched(params, configs)\n",
    "    \n",
    "    # Compute local energy\n",
    "    ratios = jnp.exp(log_psi_connected - log_psi_sigma)\n",
    "    local_energy_val = jnp.sum(matrix_elements * ratios)\n",
    "    \n",
    "    return local_energy_val\n",
    "\n",
    "# Vectorized local energy for batch\n",
    "local_energy_batched = jax.vmap(local_energy, in_axes=(None, 0, None))\n",
    "\n",
    "# Compute local energies for batch\n",
    "local_energies = local_energy_batched(params, batch_configs, hamiltonian)\n",
    "\n",
    "print(f\"Local energy computation:\")\n",
    "print(f\"  Mean E_loc: {jnp.mean(local_energies):.6f}\")\n",
    "print(f\"  Std E_loc: {jnp.std(local_energies):.6f}\")\n",
    "print(f\"  Energy estimate: {jnp.mean(local_energies):.6f} ± {jnp.std(local_energies)/jnp.sqrt(len(local_energies)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5556c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient computation for variational optimization\n",
    "def energy_function(params, configs, hamiltonian):\n",
    "    \"\"\"Energy function for optimization\"\"\"\n",
    "    local_energies = local_energy_batched(params, configs, hamiltonian)\n",
    "    return jnp.mean(local_energies)\n",
    "\n",
    "# Compute gradient\n",
    "energy_grad = jax.grad(energy_function)\n",
    "\n",
    "# Evaluate gradient\n",
    "gradient = energy_grad(params, batch_configs, hamiltonian)\n",
    "\n",
    "print(f\"Gradient computation:\")\n",
    "print(f\"  Parameter gradients computed successfully\")\n",
    "for name, grad in gradient['params'].items():\n",
    "    grad_norm = jnp.linalg.norm(grad.flatten())\n",
    "    print(f\"  {name}: gradient norm = {grad_norm:.6f}\")\n",
    "\n",
    "# JIT-compiled version for performance\n",
    "energy_and_grad_jit = jax.jit(jax.value_and_grad(energy_function))\n",
    "\n",
    "# Benchmark\n",
    "n_runs = 10\n",
    "\n",
    "# Regular computation\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    energy_val = energy_function(params, batch_configs, hamiltonian)\n",
    "    grad_val = energy_grad(params, batch_configs, hamiltonian)\n",
    "time_regular = time.time() - start\n",
    "\n",
    "# JIT-compiled computation (warm up first)\n",
    "_ = energy_and_grad_jit(params, batch_configs, hamiltonian)\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    energy_val_jit, grad_val_jit = energy_and_grad_jit(params, batch_configs, hamiltonian)\n",
    "time_jit = time.time() - start\n",
    "\n",
    "print(f\"\\nPerformance comparison ({n_runs} runs):\")\n",
    "print(f\"  Regular: {time_regular:.4f} seconds\")\n",
    "print(f\"  JIT: {time_jit:.4f} seconds\")\n",
    "print(f\"  Speedup: {time_regular / time_jit:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dabe20",
   "metadata": {},
   "source": [
    "## 9. Advanced Topics {#advanced}\n",
    "\n",
    "Advanced JAX features useful for quantum many-body calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6600cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for iterative computations\n",
    "def rk4_step(state, dt_and_force):\n",
    "    \"\"\"Single RK4 step for time evolution\"\"\"\n",
    "    y, t = state\n",
    "    dt, force_fn = dt_and_force\n",
    "    \n",
    "    k1 = dt * force_fn(y, t)\n",
    "    k2 = dt * force_fn(y + k1/2, t + dt/2)\n",
    "    k3 = dt * force_fn(y + k2/2, t + dt/2)\n",
    "    k4 = dt * force_fn(y + k3, t + dt)\n",
    "    \n",
    "    y_new = y + (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "    t_new = t + dt\n",
    "    \n",
    "    return (y_new, t_new), y_new\n",
    "\n",
    "def simulate_system(initial_state, times, force_fn):\n",
    "    \"\"\"Simulate a dynamical system using RK4\"\"\"\n",
    "    dt = times[1] - times[0]\n",
    "    dt_and_force = (dt, force_fn)\n",
    "    \n",
    "    # Use scan for efficient iteration\n",
    "    final_state, trajectory = jax.lax.scan(\n",
    "        rk4_step, \n",
    "        (initial_state, times[0]), \n",
    "        jnp.repeat(dt_and_force, len(times) - 1, axis=0)\n",
    "    )\n",
    "    \n",
    "    return jnp.concatenate([initial_state[None], trajectory])\n",
    "\n",
    "# Example: harmonic oscillator\n",
    "def harmonic_force(state, t):\n",
    "    \"\"\"Force for harmonic oscillator: d²x/dt² = -ω²x\"\"\"\n",
    "    x, v = state\n",
    "    omega = 2.0\n",
    "    return jnp.array([v, -omega**2 * x])\n",
    "\n",
    "# Simulate\n",
    "initial_state = jnp.array([1.0, 0.0])  # x=1, v=0\n",
    "times = jnp.linspace(0, 2*jnp.pi, 100)\n",
    "\n",
    "trajectory = simulate_system(initial_state, times, harmonic_force)\n",
    "\n",
    "print(f\"Time evolution simulation:\")\n",
    "print(f\"  Time points: {len(times)}\")\n",
    "print(f\"  Trajectory shape: {trajectory.shape}\")\n",
    "print(f\"  Energy conservation (should be ~constant):\")\n",
    "energy = 0.5 * (trajectory[:, 1]**2 + 4 * trajectory[:, 0]**2)\n",
    "print(f\"    Initial energy: {energy[0]:.6f}\")\n",
    "print(f\"    Final energy: {energy[-1]:.6f}\")\n",
    "print(f\"    Energy drift: {abs(energy[-1] - energy[0])/energy[0]*100:.2e}%\")\n",
    "\n",
    "# Plot trajectory\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(times, trajectory[:, 0], label='Position')\n",
    "plt.plot(times, trajectory[:, 1], label='Velocity')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Harmonic Oscillator Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1])\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Phase Space')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24862953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional computations with jax.lax.cond\n",
    "def adaptive_step_size(current_error, target_error, current_dt):\n",
    "    \"\"\"Adaptive step size control\"\"\"\n",
    "    safety_factor = 0.9\n",
    "    max_factor = 2.0\n",
    "    min_factor = 0.5\n",
    "    \n",
    "    def increase_step(args):\n",
    "        current_dt, factor = args\n",
    "        return jnp.minimum(current_dt * factor, current_dt * max_factor)\n",
    "    \n",
    "    def decrease_step(args):\n",
    "        current_dt, factor = args\n",
    "        return jnp.maximum(current_dt * factor, current_dt * min_factor)\n",
    "    \n",
    "    error_ratio = current_error / target_error\n",
    "    factor = safety_factor * (1.0 / error_ratio)**0.2\n",
    "    \n",
    "    new_dt = jax.lax.cond(\n",
    "        error_ratio < 1.0,\n",
    "        increase_step,\n",
    "        decrease_step,\n",
    "        (current_dt, factor)\n",
    "    )\n",
    "    \n",
    "    return new_dt\n",
    "\n",
    "# Test adaptive step size\n",
    "errors = jnp.array([0.001, 0.01, 0.1, 1.0, 10.0])\n",
    "target_error = 0.01\n",
    "current_dt = 0.01\n",
    "\n",
    "print(f\"Adaptive step size control:\")\n",
    "print(f\"Target error: {target_error}\")\n",
    "print(f\"Current dt: {current_dt}\")\n",
    "\n",
    "for error in errors:\n",
    "    new_dt = adaptive_step_size(error, target_error, current_dt)\n",
    "    print(f\"  Error {error:.3f} -> dt {new_dt:.6f} (factor: {new_dt/current_dt:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93930321",
   "metadata": {},
   "source": [
    "## 10. Conclusion {#conclusion}\n",
    "\n",
    "This notebook has demonstrated the key JAX features that make NetKet powerful and efficient:\n",
    "\n",
    "### JAX Features Covered:\n",
    "\n",
    "1. **Automatic Differentiation**:\n",
    "   - Essential for variational optimization\n",
    "   - Forward and reverse mode AD\n",
    "   - Higher-order derivatives\n",
    "\n",
    "2. **JIT Compilation**:\n",
    "   - Dramatic performance improvements\n",
    "   - XLA optimization\n",
    "   - Static vs dynamic shapes\n",
    "\n",
    "3. **Vectorization with vmap**:\n",
    "   - Automatic batching\n",
    "   - Eliminates explicit loops\n",
    "   - Better hardware utilization\n",
    "\n",
    "4. **Parallelization with pmap**:\n",
    "   - Multi-device computation\n",
    "   - Scales to large systems\n",
    "   - Efficient for Monte Carlo sampling\n",
    "\n",
    "5. **Advanced Features**:\n",
    "   - Custom gradients for stability\n",
    "   - Memory optimization\n",
    "   - Control flow primitives\n",
    "\n",
    "### NetKet Benefits:\n",
    "\n",
    "- **Performance**: JIT compilation and vectorization\n",
    "- **Scalability**: Automatic parallelization\n",
    "- **Flexibility**: Custom neural architectures\n",
    "- **Numerical Stability**: Custom gradient rules\n",
    "- **Memory Efficiency**: Gradient checkpointing\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use JIT compilation** for computational hot spots\n",
    "2. **Vectorize operations** with vmap instead of loops\n",
    "3. **Leverage static shapes** when possible\n",
    "4. **Use custom gradients** for numerical stability\n",
    "5. **Profile and optimize** memory usage for large systems\n",
    "\n",
    "### Impact on Quantum Many-Body Physics:\n",
    "\n",
    "JAX's features enable:\n",
    "- Efficient neural quantum state optimization\n",
    "- Large-scale Monte Carlo simulations\n",
    "- Real-time quantum dynamics\n",
    "- Custom quantum algorithms\n",
    "- Scalable quantum machine learning\n",
    "\n",
    "The combination of JAX's computational power and NetKet's quantum-specific abstractions makes it possible to tackle previously intractable quantum many-body problems with neural network ansatzes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
