{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e2e1d5",
   "metadata": {},
   "source": [
    "# Neural Quantum States (NQS) with NetKet\n",
    "\n",
    "This notebook provides a comprehensive exploration of Neural Network representations of quantum states using NetKet. Neural Quantum States (NQS) are a powerful approach to represent quantum many-body systems using artificial neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Neural Quantum States](#introduction)\n",
    "2. [Setting up NetKet](#setup)\n",
    "3. [Basic NQS: Restricted Boltzmann Machine](#rbm)\n",
    "4. [Feed-Forward Neural Networks](#ffnn)\n",
    "5. [Complex-valued Neural Networks](#complex)\n",
    "6. [Autoregressive Neural Networks](#autoregressive)\n",
    "7. [Performance Comparison](#comparison)\n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c54ac",
   "metadata": {},
   "source": [
    "## 1. Introduction to Neural Quantum States {#introduction}\n",
    "\n",
    "Neural Quantum States represent quantum many-body wavefunctions using neural networks. For a quantum state $|\\psi\\rangle$, the NQS ansatz is:\n",
    "\n",
    "$$\\psi(\\sigma) = \\langle \\sigma | \\psi \\rangle = \\mathcal{N} \\exp\\left(\\sum_i \\ln f_i(\\sigma; \\theta)\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\sigma$ represents a configuration in the computational basis\n",
    "- $f_i(\\sigma; \\theta)$ are neural network outputs parameterized by $\\theta$\n",
    "- $\\mathcal{N}$ is a normalization constant\n",
    "\n",
    "### Key Advantages:\n",
    "- **Expressivity**: Can represent complex quantum states\n",
    "- **Scalability**: Polynomial scaling with system size\n",
    "- **Flexibility**: Can incorporate physical symmetries\n",
    "- **Optimization**: Leverages automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80660ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetKet version: 3.19.2\n",
      "JAX version: 0.5.3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import netket as nk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "jax.config.update('jax_platform_name', 'cpu')  # Use CPU for this tutorial\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "print(f\"NetKet version: {nk.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f338fed",
   "metadata": {},
   "source": [
    "## 2. Setting up NetKet {#setup}\n",
    "\n",
    "Let's start by setting up a simple quantum system - the 1D Ising model with transverse field:\n",
    "\n",
    "$$H = -J \\sum_{i} \\sigma^z_i \\sigma^z_{i+1} - h \\sum_i \\sigma^x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931e601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System size: 8 spins\n",
      "Hilbert space dimension: 256\n",
      "Hamiltonian: IsingJax(J=1.0, h=0.5; dim=8)\n"
     ]
    }
   ],
   "source": [
    "# Define the lattice\n",
    "N = 8  # Number of spins\n",
    "lattice = nk.graph.Chain(length=N, pbc=True)  # Periodic boundary conditions\n",
    "\n",
    "# Define the Hilbert space (spin-1/2)\n",
    "hilbert = nk.hilbert.Spin(s=1/2, N=lattice.n_nodes)\n",
    "\n",
    "# Define the Hamiltonian\n",
    "J = 1.0  # Coupling strength\n",
    "h = 0.5  # Transverse field strength\n",
    "\n",
    "# Ising interaction\n",
    "hamiltonian = nk.operator.Ising(hilbert=hilbert, graph=lattice, J=J, h=h)\n",
    "\n",
    "print(f\"System size: {N} spins\")\n",
    "print(f\"Hilbert space dimension: {hilbert.n_states}\")\n",
    "print(f\"Hamiltonian: {hamiltonian}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9280fb",
   "metadata": {},
   "source": [
    "## 3. Basic NQS: Restricted Boltzmann Machine {#rbm}\n",
    "\n",
    "The Restricted Boltzmann Machine (RBM) is one of the simplest and most successful NQS architectures. It consists of visible and hidden units:\n",
    "\n",
    "$$\\psi_{\\text{RBM}}(\\sigma) = \\exp\\left(\\sum_i a_i \\sigma_i + \\sum_j b_j h_j + \\sum_{ij} W_{ij} \\sigma_i h_j\\right)$$\n",
    "\n",
    "where the hidden units are marginalized: $h_j = \\tanh(b_j + \\sum_i W_{ij} \\sigma_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec90d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RBM.__init__() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m alpha = \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Hidden unit density (hidden units = alpha * visible units)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# RBM with real-valued parameters\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m rbm_real = \u001b[43mnk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRBM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_visible_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hidden_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRBM Architecture:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- Visible units: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\satyam\\Desktop\\research-project\\GenPhase\\.venv\\Lib\\site-packages\\flax\\linen\\kw_only_dataclasses.py:235\u001b[39m, in \u001b[36m_process_class.<locals>.init_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_args > expected_num_args:\n\u001b[32m    228\u001b[39m   \u001b[38;5;66;03m# we add + 1 to each to account for `self`, matching python's\u001b[39;00m\n\u001b[32m    229\u001b[39m   \u001b[38;5;66;03m# default error message\u001b[39;00m\n\u001b[32m    230\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    231\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m__init__() takes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_num_args\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m positional \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33marguments but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_args\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were given\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    233\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[43mdataclass_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: RBM.__init__() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "# Create an RBM-based Neural Quantum State\n",
    "alpha = 2  # Hidden unit density (hidden units = alpha * visible units)\n",
    "\n",
    "# RBM with default parameters (dtype is handled internally)\n",
    "rbm_real = nk.models.RBM(\n",
    "    alpha=alpha,\n",
    "    use_visible_bias=True,\n",
    "    use_hidden_bias=True\n",
    ")\n",
    "\n",
    "print(f\"RBM Architecture:\")\n",
    "print(f\"- Visible units: {N}\")\n",
    "print(f\"- Hidden units: {alpha * N}\")\n",
    "print(f\"- Total parameters: {N + alpha * N + N * alpha * N}\")\n",
    "\n",
    "# Initialize the model with proper input shape\n",
    "key, subkey = jax.random.split(key)\n",
    "sample_input = hilbert.random_state(subkey, size=1)\n",
    "\n",
    "# Initialize parameters correctly\n",
    "try:\n",
    "    rbm_params = rbm_real.init(subkey, sample_input)\n",
    "    print(f\"\\nModel initialization successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Standard initialization failed: {e}\")\n",
    "    # Try alternative initialization\n",
    "    rbm_params = rbm_real.init({'params': subkey}, sample_input)\n",
    "    print(f\"Alternative initialization successful!\")\n",
    "\n",
    "print(f\"\\nParameter structure:\")\n",
    "try:\n",
    "    if 'params' in rbm_params:\n",
    "        for name, param in rbm_params['params'].items():\n",
    "            print(f\"- {name}: {param.shape} (dtype: {param.dtype})\")\n",
    "    else:\n",
    "        # Handle different parameter structure\n",
    "        for name, param in rbm_params.items():\n",
    "            if hasattr(param, 'shape'):\n",
    "                print(f\"- {name}: {param.shape} (dtype: {param.dtype})\")\n",
    "            else:\n",
    "                print(f\"- {name}: {type(param)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Parameter inspection failed: {e}\")\n",
    "\n",
    "# Verify the model works\n",
    "try:\n",
    "    test_output = rbm_real.apply(rbm_params, sample_input)\n",
    "    print(f\"\\nModel test successful. Output shape: {test_output.shape}, dtype: {test_output.dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nModel test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate RBM evaluation\n",
    "# Generate some sample configurations\n",
    "key, subkey = jax.random.split(key)\n",
    "sample_configs = hilbert.random_state(subkey, size=5)\n",
    "\n",
    "print(\"Sample configurations and their RBM log-amplitudes:\")\n",
    "print(f\"Sample configs shape: {sample_configs.shape}\")\n",
    "print(f\"Sample configs dtype: {sample_configs.dtype}\")\n",
    "\n",
    "try:\n",
    "    log_psi = rbm_real.apply(rbm_params, sample_configs)\n",
    "    \n",
    "    print(f\"RBM output shape: {log_psi.shape}\")\n",
    "    print(f\"RBM output dtype: {log_psi.dtype}\")\n",
    "    \n",
    "    for i, (config, log_amp) in enumerate(zip(sample_configs, log_psi)):\n",
    "        print(f\"Config {i+1}: {config} -> log|ψ| = {log_amp:.4f}\")\n",
    "    \n",
    "    # The probability amplitude\n",
    "    print(f\"\\nProbability amplitudes |ψ|²:\")\n",
    "    probs = jnp.exp(2 * jnp.real(log_psi))\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"Config {i+1}: |ψ|² = {prob:.6f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in RBM evaluation: {e}\")\n",
    "    print(\"Debugging - trying individual configurations:\")\n",
    "    \n",
    "    # Try with individual configurations\n",
    "    for i, config in enumerate(sample_configs):\n",
    "        try:\n",
    "            # Ensure proper shape for single configuration\n",
    "            single_config = config.reshape(1, -1) if len(config.shape) == 1 else config\n",
    "            log_amp = rbm_real.apply(rbm_params, single_config)\n",
    "            print(f\"Config {i+1}: {config} -> log|ψ| = {log_amp[0]:.4f}\")\n",
    "        except Exception as inner_e:\n",
    "            print(f\"Config {i+1} failed: {inner_e}\")\n",
    "            # Try without reshaping\n",
    "            try:\n",
    "                log_amp = rbm_real.apply(rbm_params, config)\n",
    "                print(f\"Config {i+1} (no reshape): {config} -> log|ψ| = {log_amp:.4f}\")\n",
    "            except Exception as inner_e2:\n",
    "                print(f\"Config {i+1} (no reshape) also failed: {inner_e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a19f11",
   "metadata": {},
   "source": [
    "## 4. Feed-Forward Neural Networks {#ffnn}\n",
    "\n",
    "Feed-forward neural networks can provide more expressive representations than RBMs. In NetKet, we can easily create custom architectures using Flax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Feed-Forward Neural Network for quantum states.\n",
    "    \"\"\"\n",
    "    layers: tuple = (32, 32, 16)\n",
    "    activation: Callable = nn.relu\n",
    "    dtype: Any = jnp.complex128\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Convert spin configuration to real values\n",
    "        x = jnp.array(x, dtype=jnp.float64)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for layer_size in self.layers:\n",
    "            x = nn.Dense(layer_size, dtype=self.dtype)(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        # Output layer (single complex number)\n",
    "        x = nn.Dense(1, dtype=self.dtype)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "\n",
    "# Create the deep neural network\n",
    "deep_nn = DeepFFNN(layers=(16, 16, 8), dtype=jnp.complex128)\n",
    "\n",
    "# Initialize parameters\n",
    "key, subkey = jax.random.split(key)\n",
    "deep_params = deep_nn.init(subkey, sample_input[0])\n",
    "\n",
    "print(f\"Deep FFNN Architecture:\")\n",
    "total_params = 0\n",
    "for name, param in deep_params['params'].items():\n",
    "    param_count = np.prod(param.shape)\n",
    "    total_params += param_count\n",
    "    print(f\"- {name}: {param.shape} ({param_count} parameters)\")\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deep neural network\n",
    "print(\"Deep FFNN evaluation on sample configurations:\")\n",
    "log_psi_deep = deep_nn.apply(deep_params, sample_configs)\n",
    "\n",
    "for i, (config, log_amp) in enumerate(zip(sample_configs, log_psi_deep)):\n",
    "    print(f\"Config {i+1}: {config} -> log ψ = {log_amp:.4f}\")\n",
    "\n",
    "# Compare magnitude with RBM\n",
    "print(f\"\\nComparison of |log ψ| magnitudes:\")\n",
    "print(f\"RBM: {jnp.abs(log_psi).mean():.4f} ± {jnp.abs(log_psi).std():.4f}\")\n",
    "print(f\"Deep NN: {jnp.abs(log_psi_deep).mean():.4f} ± {jnp.abs(log_psi_deep).std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89e1d9",
   "metadata": {},
   "source": [
    "## 5. Complex-valued Neural Networks {#complex}\n",
    "\n",
    "For quantum states, we often need complex-valued wavefunctions. NetKet supports complex neural networks naturally through JAX's complex number support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex RBM - also remove dtype parameter\n",
    "rbm_complex = nk.models.RBM(\n",
    "    alpha=alpha,\n",
    "    use_visible_bias=True,\n",
    "    use_hidden_bias=True\n",
    ")\n",
    "\n",
    "# Initialize with complex parameters\n",
    "key, subkey = jax.random.split(key)\n",
    "try:\n",
    "    rbm_complex_params = rbm_complex.init(subkey, sample_input)\n",
    "    \n",
    "    # Evaluate complex amplitudes\n",
    "    log_psi_complex = rbm_complex.apply(rbm_complex_params, sample_configs)\n",
    "    \n",
    "    print(\"Complex RBM evaluation:\")\n",
    "    for i, (config, log_amp) in enumerate(zip(sample_configs, log_psi_complex)):\n",
    "        # Handle both real and complex outputs\n",
    "        if jnp.iscomplexobj(log_amp):\n",
    "            phase = jnp.angle(jnp.exp(log_amp))\n",
    "            magnitude = jnp.abs(jnp.exp(log_amp))\n",
    "            print(f\"Config {i+1}: log ψ = {log_amp:.4f}, |ψ| = {magnitude:.4f}, phase = {phase:.4f}\")\n",
    "        else:\n",
    "            magnitude = jnp.exp(log_amp)\n",
    "            print(f\"Config {i+1}: log ψ = {log_amp:.4f}, |ψ| = {magnitude:.4f} (real)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Complex RBM failed: {e}\")\n",
    "    print(\"This version may not support complex RBMs directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467a67f",
   "metadata": {},
   "source": [
    "## 6. Autoregressive Neural Networks {#autoregressive}\n",
    "\n",
    "Autoregressive models factorize the wavefunction as:\n",
    "$$\\psi(\\sigma_1, \\ldots, \\sigma_N) = \\prod_{i=1}^N \\psi_i(\\sigma_i | \\sigma_1, \\ldots, \\sigma_{i-1})$$\n",
    "\n",
    "This approach can be very expressive and allows for exact sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b995622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an autoregressive neural network\n",
    "# For simplicity, we'll use NetKet's FastARNN\n",
    "try:\n",
    "    autoregressive_nn = nk.models.FastARNN(\n",
    "        hilbert=hilbert,\n",
    "        layers=2,\n",
    "        features=8,\n",
    "        dtype=jnp.complex128\n",
    "    )\n",
    "    \n",
    "    # Initialize parameters\n",
    "    key, subkey = jax.random.split(key)\n",
    "    ar_params = autoregressive_nn.init(subkey, sample_input)\n",
    "    \n",
    "    # Evaluate\n",
    "    log_psi_ar = autoregressive_nn.apply(ar_params, sample_configs)\n",
    "    \n",
    "    print(\"Autoregressive NN evaluation:\")\n",
    "    for i, (config, log_amp) in enumerate(zip(sample_configs, log_psi_ar)):\n",
    "        print(f\"Config {i+1}: {config} -> log ψ = {log_amp:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Autoregressive model not available in this NetKet version: {e}\")\n",
    "    print(\"Using alternative implementation...\")\n",
    "    \n",
    "    # Simple alternative: factorized approach\n",
    "    class SimpleAutoregressive(nn.Module):\n",
    "        features: int = 8\n",
    "        dtype: Any = jnp.complex128\n",
    "        \n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            x = jnp.array(x, dtype=jnp.float64)\n",
    "            log_psi = 0.0\n",
    "            \n",
    "            for i in range(x.shape[-1]):\n",
    "                # Condition on previous spins\n",
    "                context = x[..., :i+1]\n",
    "                h = nn.Dense(self.features, dtype=self.dtype, name=f'dense_{i}')(context)\n",
    "                h = nn.tanh(h)\n",
    "                contrib = nn.Dense(1, dtype=self.dtype, name=f'output_{i}')(h)\n",
    "                log_psi += jnp.squeeze(contrib, axis=-1)\n",
    "            \n",
    "            return log_psi\n",
    "    \n",
    "    simple_ar = SimpleAutoregressive()\n",
    "    key, subkey = jax.random.split(key)\n",
    "    simple_ar_params = simple_ar.init(subkey, sample_input[0])\n",
    "    log_psi_simple_ar = simple_ar.apply(simple_ar_params, sample_configs)\n",
    "    \n",
    "    print(\"Simple Autoregressive evaluation:\")\n",
    "    for i, (config, log_amp) in enumerate(zip(sample_configs, log_psi_simple_ar)):\n",
    "        print(f\"Config {i+1}: {config} -> log ψ = {log_amp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084a178",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison {#comparison}\n",
    "\n",
    "Let's compare different NQS architectures in terms of expressivity and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd85d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger sample for statistics\n",
    "key, subkey = jax.random.split(key)\n",
    "large_sample = hilbert.random_state(subkey, size=1000)\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    'RBM (Real)': (rbm_real, rbm_params),\n",
    "    'RBM (Complex)': (rbm_complex, rbm_complex_params),\n",
    "    'Deep FFNN': (deep_nn, deep_params)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # Time the evaluation\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    log_psi = model.apply(params, large_sample)\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate statistics\n",
    "    log_psi_var = jnp.var(jnp.real(log_psi))\n",
    "    log_psi_mean = jnp.mean(jnp.real(log_psi))\n",
    "    \n",
    "    results[name] = {\n",
    "        'mean': log_psi_mean,\n",
    "        'variance': log_psi_var,\n",
    "        'eval_time': eval_time,\n",
    "        'log_psi': log_psi\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"Model Comparison (1000 evaluations):\")\n",
    "print(\"-\" * 60)\n",
    "for name, stats in results.items():\n",
    "    print(f\"{name:15} | Mean: {stats['mean']:8.4f} | Var: {stats['variance']:8.4f} | Time: {stats['eval_time']:6.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03195d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (name, stats) in enumerate(results.items()):\n",
    "    axes[i].hist(jnp.real(stats['log_psi']), bins=50, alpha=0.7, density=True)\n",
    "    axes[i].set_title(f'{name}\\nlog|ψ| Distribution')\n",
    "    axes[i].set_xlabel('log|ψ|')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter count comparison\n",
    "print(\"\\nParameter Count Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "param_counts = {\n",
    "    'RBM': N + alpha * N + N * alpha * N,\n",
    "    'Deep FFNN': total_params\n",
    "}\n",
    "\n",
    "for name, count in param_counts.items():\n",
    "    print(f\"{name:15}: {count:6d} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6b927",
   "metadata": {},
   "source": [
    "## 8. Symmetries and Physical Constraints {#symmetries}\n",
    "\n",
    "Real quantum systems often have symmetries that can be incorporated into NQS for better performance and physical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80133bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Translation-invariant RBM\n",
    "# For periodic systems, we can enforce translation symmetry\n",
    "\n",
    "class TranslationInvariantRBM(nn.Module):\n",
    "    \"\"\"\n",
    "    RBM with enforced translation symmetry.\n",
    "    \"\"\"\n",
    "    alpha: int = 1\n",
    "    dtype: Any = jnp.complex128\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        N = x.shape[-1]\n",
    "        \n",
    "        # Visible bias (same for all sites)\n",
    "        a = self.param('visible_bias', nn.initializers.normal(stddev=0.1), (1,), self.dtype)\n",
    "        visible_contrib = a[0] * jnp.sum(x, axis=-1)\n",
    "        \n",
    "        # Hidden units with translation symmetry\n",
    "        # Each hidden unit connects to a local pattern that's translated\n",
    "        W = self.param('weights', nn.initializers.normal(stddev=0.1), (N, self.alpha), self.dtype)\n",
    "        b = self.param('hidden_bias', nn.initializers.normal(stddev=0.1), (self.alpha,), self.dtype)\n",
    "        \n",
    "        hidden_activations = []\n",
    "        for shift in range(N):\n",
    "            # Translate the weights\n",
    "            x_shifted = jnp.roll(x, shift, axis=-1)\n",
    "            for j in range(self.alpha):\n",
    "                activation = b[j] + jnp.sum(W[:, j] * x_shifted, axis=-1)\n",
    "                hidden_activations.append(jnp.log(jnp.cosh(activation)))\n",
    "        \n",
    "        hidden_contrib = jnp.sum(jnp.array(hidden_activations), axis=0)\n",
    "        \n",
    "        return visible_contrib + hidden_contrib\n",
    "\n",
    "# Create and test the symmetric RBM\n",
    "sym_rbm = TranslationInvariantRBM(alpha=1)\n",
    "key, subkey = jax.random.split(key)\n",
    "sym_params = sym_rbm.init(subkey, sample_input[0])\n",
    "\n",
    "# Test translation invariance\n",
    "test_config = jnp.array([1, -1, 1, -1, 1, -1, 1, -1])\n",
    "shifted_config = jnp.roll(test_config, 1)\n",
    "\n",
    "log_psi_orig = sym_rbm.apply(sym_params, test_config)\n",
    "log_psi_shifted = sym_rbm.apply(sym_params, shifted_config)\n",
    "\n",
    "print(f\"Original config: {test_config}\")\n",
    "print(f\"Shifted config:  {shifted_config}\")\n",
    "print(f\"log ψ (original): {log_psi_orig:.6f}\")\n",
    "print(f\"log ψ (shifted):  {log_psi_shifted:.6f}\")\n",
    "print(f\"Difference: {abs(log_psi_orig - log_psi_shifted):.8f}\")\n",
    "print(f\"Translation invariant: {abs(log_psi_orig - log_psi_shifted) < 1e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d196d",
   "metadata": {},
   "source": [
    "## 9. Conclusion {#conclusion}\n",
    "\n",
    "In this notebook, we've explored various Neural Quantum State architectures in NetKet:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **RBMs** are simple, effective, and well-studied NQS architectures\n",
    "2. **Deep neural networks** offer more expressivity but require more parameters\n",
    "3. **Complex-valued networks** are essential for representing quantum phases\n",
    "4. **Autoregressive models** provide exact sampling capabilities\n",
    "5. **Symmetries** can be incorporated for better physical accuracy and efficiency\n",
    "\n",
    "### Practical Considerations:\n",
    "\n",
    "- **Parameter count**: More parameters don't always mean better performance\n",
    "- **Initialization**: Proper initialization is crucial for training stability\n",
    "- **Architecture choice**: Depends on the specific quantum system and properties of interest\n",
    "- **Computational cost**: Balance between expressivity and evaluation speed\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore variational optimization with these NQS (covered in VMC notebook)\n",
    "- Investigate ground state and excited state calculations\n",
    "- Study quantum phase transitions\n",
    "- Implement custom architectures for specific problems\n",
    "\n",
    "The flexibility of NetKet's design allows for easy experimentation with different NQS architectures, making it an ideal platform for quantum many-body research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
